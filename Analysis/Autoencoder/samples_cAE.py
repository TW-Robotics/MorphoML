# This code is available under a GPL v3.0 license and comes without
# any explicit or implicit warranty.
#
# (C) Wilfried Woeber 2021 <wilfried.woeber@technikum-wien.at>
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras import models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import matplotlib.pyplot as plt
import sys
import os
#--- Manage physical memory ---#
physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)
#------------------------------#
#--- Define global settings ---#
#------------------------------#
##TODO: Store parameters in config file
train_dir=sys.argv[1]       #Train folder generated by bash script
img_sizeX = int(sys.argv[3])     #Image size in X
img_sizeY = int(sys.argv[4])     #Image size in Y
train_batchsize = 8         #Used batches for training
val_batchsize = 5           #Validation batchsize
#-----------------------#
#--- Load best model ---#
#-----------------------#
best_model_path=sys.argv[2] #Get path to best model csv file
#------------------#
#--- Load model ---#
#------------------#
model = models.load_model(best_model_path+"/my_model")
#---------------#
#--- Use cAE ---#
#---------------#
train_im = ImageDataGenerator(rescale=1./255)
def test_images():
    train_generator = train_im.flow_from_directory (
            train_dir, 
            target_size=(img_sizeX,img_sizeY),
            color_mode='rgb',
            batch_size=1000,
            shuffle = False,
            class_mode='categorical'
         )
    x =  train_generator
    return x[0][0], x[0][1], train_generator.filenames
test_data = test_images()
encoder = Model(inputs=model.input, outputs=model.get_layer('Code').output)
prediction = encoder.predict(test_data[0])
decoder = models.Sequential()
for layer in model.layers[21:len(model.layers)+1]:
    decoder.add(layer)
#imgs = decoder.predict(prediction) #Not needed
#--------------------------------#
#--- Sample from latent space ---#
#--------------------------------#
VarMemory = np.zeros((prediction.shape[1],img_sizeX,img_sizeY))   #Memory for variance memory
for i in range(0,prediction.shape[1]):  #For each latent dimension
    print("Process code dimension %d" % i)
    feature = np.zeros((1,prediction.shape[1]))         #Init memory
    for k in range(0,prediction.shape[1]):              #For all dimensions, estimate mean value
        feature[0,k] = np.mean(prediction[:,k])         #(get mean value)
    #mx_val = np.max(prediction[:,i])                    #Get maximum of chosen dimension
    #mn_val = np.min(prediction[:,i])                    #Get minimum of chosen dimension
    featureVar_memory = np.zeros((prediction.shape[0],img_sizeX,img_sizeY))   #Memory for variance
    looper=0
    #--- Now we use real sample data ---#
    for k in range(0,prediction.shape[0]): #For all samples
        f_vector = feature                                      #Get mean feature
        f_vector[0,i]=prediction[k,i]                           #Replace chosen dimension with value
        img_pred = decoder.predict(f_vector)                    #Reproject to image space
        ##TODO Convert from RGB to grayscale
        img_pred =  (0.21 * img_pred[:,:,:,:1]) + (0.72 * img_pred[:,:,:,1:2]) +  (0.07 * img_pred[:,:,:,-1:])
        featureVar_memory[k,:,:]=img_pred[0,:,:,0]              #Store reprojected and added value
        looper=looper+1                                         #Increment looper
    VarMemory[i,:,:]=np.var(featureVar_memory,0)
#----------------------#
#--- Store heatmaps ---#
#----------------------#
os.system("mkdir Heatmaps")
for i in range(0,prediction.shape[1]):
    plt.close()
    plt.imshow(VarMemory[i,:,:])
    plt.axis('off')
    plt.savefig("Heatmaps/Heatmap_"+str(i)+".png")
    np.savetxt( "Heatmaps/Heatmap_"+str(i)+".csv", VarMemory[i,:,:], delimiter=',')
